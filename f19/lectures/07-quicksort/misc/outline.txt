LAST-1: selection sort -- O(n^2)
LAST: divide (& conquer) -- search from O(n) to O(log n)
TODAY:
- divide & conquer (speeding up sorting)
- recursion
- randomness
NEXT: start of part II of the course (data structures)

Important concepts (5 = highest, 1 = lowest)
============================================
[5] divide and conquer
[5] recursive algorithms
[5] merge prototype
[2] merge
[5] mergesort
[4] complexity of mergesort
[5] partition prototype
[2] partition
[5] quicksort
[4] choice of pivot
[4] average case cost analysis
[3] stable sorting

Review
======
- linear search  -- O(n)
- selection sort -- O(n^2)
- binary search  -- O(log n) ... much more efficient!

Reflections on binary search
============================
- we spent a lot of time to write a tiny amount of code, all 200 of us together
- but it compiled and run correctly right off the bat
  . deliberate programming
- this is the amount of time and effort it takes to write good code
  Remember that when
  . you keep on having bugs in your program (you are going too fast)
  . you feel like you are spending too much time on a programming hw
    (you may be doing it just right)


Sorting using divide & conquer -- part I
========================================
- linear to binary search: O(n) -> O(log n) by dividing the problem in half each time
  . can we do the same with sorting?
- what is the cost of selection sort on an n-element array?  O(n^2)
  . is this good?  Google sorts the web to display the result of a search.  Say there are 1
    billion (=10^9) pages and examining 1 page takes 1 nanosecond (10^-9 seconds).  It would
    take 10^9 seconds = more than 3 years!
- cost of selection sort on the first half? 1/4 of that.  Same for the other half
- sorting 2 halves separately saves us 1/2 of the work!
  . divide, just like binary search but done twice
- if we can combine them cheaply,  we have a better algorithm
  . conquer
       lo          mid        hi
      -----------------------
    A |          |          |         (input size: n = hi - lo)
      -----------------------
                                  2x selection sort
      -----------------------
    A |  sorted  |  sorted  |         (cost: (n/2)^2 + (n/2)^2 = n^2/2)
      -----------------------
                                  merge
      -----------------------
    A |        sorted       |
      -----------------------

- code so far
 1.   void selection_sort(int[] A, int lo, int hi)
 2.   //@requires 0 <= lo && lo <= hi && hi <= \length(A);
 3.   //@ensures is_sorted(A, lo, hi);
 4.   ;
 5.
 6.   void sort(int[] A, int lo, int hi)
 7.   //@requires 0 <= lo && lo <= hi && hi <= \length(A);
 8.   //@ensures is_sorted(A, lo, hi);
 9.   {
10.     int mid = lo + (hi - lo)/2;
11.     //@assert ...
12.     selection_sort(A, lo, mid);    //@assert is_sorted(A, lo, mid);
13.     selection_sort(A, mid, hi);    //@assert is_sorted(A, mid, hi);
14.     // ...
      }

- //@assert on line 11?
  . for binary sort, was lo <= mid && mid < hi
  . but we were inside a loop with guard lo < hi
  . here lo == hi is Ok
  . so update to
11.     //@assert lo <= mid && mid <= hi;

merge
=====
- to finish it off, we need a function that turns 2 sorted half arrays into a sorted whole: merge
- contracts?
    void merge(int[] A, int lo, int mid, int hi)
    //@requires 0 <= lo && lo <= mid && mid <= hi && hi <= \length(A);
    //@requires is_sorted(A, lo, mid) && is_sorted(A, mid, hi);
    //@ensures is_sorted(A, lo, hi);
- how would we implement it intuitively
  . run small example: A = [3, 6, 7 | 2, 2, 5]
  . compare fronts of the two halves, store smallest element into temporary array, repeat
  . copy temporary array back into A
  . requires a temporary array: merge is NOT in-place
  . see code online for actual implementation
    [expect lots of questions/variants]
- complexity of merge?  O(n)

Back to sort
============
- add line
14.     merge(A, lo, mid, hi);  //@assert is_sorted(A, lo, hi);
- check that contracts work
  . sort is (partially) correct
- benefit?
  . half of O(n^2) + O(n)
  . still O(n^2) but feels like a lot less work

Towards mergesort
=================
- selection_sort and sort have the exact same contracts!
- magic: if we replace calls to selection_sort to calls to sort, we cannot get the wrong answer
- do it and check contracts again
- this is a recursive implementation: no loop, no loop invariants!
  . INIT: safety of initial call to the function
  . PRES: from preconditions of function to safety of recursive calls
  . EXIT: from postconditions of recursive calls to postconditions of function
- can anything go wrong?
  . there is no base case!
  . code does not terminate!
    . if hi = lo, then mid = lo and first recursive call on exact same arguments
      fix with base case
        if (lo == hi) return;
    . if hi = lo+1, then mid = lo still
      first recursive call handled by base case but second has exact same arguments
      fix with second base case:
        if (lo == hi - 1) return;
- clean up code by combining base cases:
    if (hi - lo <= 1) return;
- is this correct?
  . yes, 0- and 1-element arrays are always sorted
- does it terminate?
  . at each recursive call hi - lo (array segment) gets smaller

mergesort
=========
void mergesort(int[] A, int lo, int hi)
//@requires 0 <= lo && lo <= hi && hi <= \length(A);
//@ensures is_sorted(A, lo, hi);
{
  if (hi - lo <= 1) return;

  int mid = lo + (hi - lo)/2;
  //@assert lo < mid && mid < hi; // UPDATED
  mergesort(A, lo, mid);          //@assert is_sorted(A, lo, mid);
  mergesort(A, mid, hi);          //@assert is_sorted(A, mid, hi);
  merge(A, lo, mid, hi);          //@assert is_sorted(A, lo, hi);
}

Complexity
==========
- all the real work is done by merge, which is O(n)
  . 1 call  on n-element   array segment:  n
  . 2 calls on n/2 element array segments: n/2 + n/2 = n
  . 4 calls on n/4 element array segments: 4 * n/4 = n
  . ...
  . lots of calls on 0 or 1 element array segments: no call to merge
      -------------------------
      |  |  |  |  |  |  |  |  |
      -------------------------
      |     |     |     |     |
      -------------------------
      |           |           |
      -------------------------
      |                       |
      -------------------------
- how many levels are there?  log n
- log n times O(n) work is O(n log n)
- cost of mergesort is O(n log n)  -- much better than selection sort!

Reflection
==========
- structure of mergesort
  . minimal work before: divide array (computation of mid, O(1))
  . recursive calls in middle
  . some work after: merge sorted arrays O(n)
- what about the other way around?
  . minimal/no work after: segments are sorted and in right order -- O(1)
  . recursive calls in middle
  . some work before: arrange so that segments are in the right order (partition)
  => idea of quicksort

Alternative motivation
======================
- linear sort (look at next element) --d&c--> binary sort (look at midpoint)
- selection sort (fill next slot with right element) --d&c--> (fill midpoint with right element,
   and work on both side)
  . move smaller elements before and larger elements after: partition
  . repeat
  . that's quicksort


Quicksort
=========
       lo                                 hi
      -----------------------------------
    A |                          |X|    |
      -----------------------------------
                                                partition
       lo          p                      hi
      -----------------------------------
    A |           |X|                   |
      -----------------------------------
        A[lo,p) <= X <= A[p+1,hi)
                                                2x quicksort
      -----------------------------------
    A |  sorted   |X|    sorted         |
      -----------------------------------

void quicksort(int[] A, int lo, int hi)
//@requires 0 <= lo && lo <= hi && hi <= \length(A);
//@ensures is_sorted(A, lo, hi);
{
  if (hi - lo <= 1) return;      // we learned that from mergesort

  int p = partition(A, lo, hi);  //@assert lo <= p && p < hi;

  //@assert ge_seg(A[p], A, lo, p) && le_seg(A[p], A, p+1, hi);
  quicksort(A, lo, p);           //@assert is_sorted(A, lo, p);
  quicksort(A, p+1, hi);         //@assert is_sorted(A, p+1, hi);
}
- just like we passed mid to merge, we expect mid (= p) from partition
- ideally, p should just be the midpoint
  . element that belongs in midpoint is the median
  . how to find it?
    . sort array and then median is midpoint: silly and O(n log n)
    . use O(n) approximate algorithm (expensive in practice)
    . give up on requirement that's the midpoint
      . p is the pivot index
      . element that ends up there is the pivot -- always in the right place
      . all elements to left are <= and all elements to right are >=
- correctness
  . A[lo,p) sorted and A[p+1,hi] sorted and A[lo,p) <= A[p] <= A[p+1, hi)
    implies A[lo,hi) sorted

partition
=========
int partition(int[] A, int lo, int hi)
//@requires 0 <= lo && lo < hi && hi <= \length(A);
//@ensures lo <= \result && \result < hi;
//@ensures ge_seg(A[\result], A, lo, \result);
//@ensures le_seg(A[\result], A, \result+1, hi);
- point out lo < hi and \result+1
- cost is O(n), can be done in-place
- see lecture notes or code online

Choice of pivot and cost
========================
- best: partition always returns midpoint (= chooses median as pivot)
  . then cost is O(n log n) -- same analysis as merge sort
  . but impractical
- worst: partition always return index of minimal element
  . degenerates to selection sort
  . O(n^2)
- in practice:
  . always return lo (or any fixed index) in [lo, hi)
  . returns a random index in [lo,hi)
  . pick 3 indices (e.g., lo, mid and hi-1 or 3 random indices) and returns that of their median
  . in all cases,
    . worst case is still O(n^2) but we have to be really unlucky
      . asymptotically small probability
    . average case is O(n log n) -- add up all possible ways to choose the pivot and divide
      by the number of strategies (15-210 material)
- how to implement partition?
  . intuitive way
    . pick pivot
    . copy all elements <= pivot in temp array  (1st pass on A)
    . copy pivot to temp
    . copy all elements > pivot in temp         (2nd pass on A)
    . copy temp back into A
    . O(n), but not in-place
  . see lecture notes
    . still O(n) and in-place [interesting invariants]

Comparing sorting algorithms (fill table as we go along)
============================
- worst time complexity
- average time complexity
- in-place? allocates temporary array
- stable: relative order of duplicate elements is unchanged
  . good when sorting records by different fields over time (e.g.,
    roster by andrew ID and then by section)

                selection sort   merge sort   quicksort
worst-case          O(n^2)       O(n log n)     O(n^2)
average-case        O(n^2)       O(n log n)   O(n log n)
in-place?              Y             N            Y
stable?                N             Y            N

- extend this table as you learn about new sorting algorithms
